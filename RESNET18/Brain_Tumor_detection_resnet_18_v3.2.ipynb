{
 "cells": [
  {
   "cell_type": "markdown",
   "source": [
    "# 腦部腫瘤分類（Pytorch）"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "## 準備動作（導入模組）"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch.utils.data import DataLoader\n",
    "import torchvision\n",
    "from torchvision import datasets\n",
    "import torchvision.transforms as transforms\n",
    "import matplotlib.pyplot as plt\n",
    "from torchsummary import summary\n",
    "\n",
    "from sklearn.metrics import accuracy_score,classification_report\n",
    "\n",
    "import pandas as pd\n",
    "from tqdm import tqdm_notebook as tqdm\n",
    "import time\n",
    "import warnings\n",
    "from collections import OrderedDict\n",
    "# 資料預處理套件\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "import os\n",
    "from tensorflow.keras.preprocessing.image import img_to_array\n",
    "from tensorflow.keras.preprocessing.image import load_img\n",
    "from tensorflow.keras.applications.mobilenet_v2 import preprocess_input\n",
    "import numpy as np\n",
    "warnings.simplefilter(\"ignore\")\n",
    "# download the pretrained model\n",
    "import torchvision.models as models"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "## 定義數據集處理的方法"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "source": [
    "def images_transforms(phase, image_size):\n",
    "    if phase == 'training':\n",
    "        data_transformation = transforms.Compose([\n",
    "            transforms.Resize(image_size),\n",
    "#             transforms.RandomEqualize(10),\n",
    "            transforms.RandomRotation(degrees=(-25,20)),\n",
    "            transforms.ToTensor(),\n",
    "            transforms.Normalize([0.485, 0.456, 0.406],[0.229, 0.224, 0.225])\n",
    "        ])\n",
    "    else:\n",
    "        data_transformation=transforms.Compose([\n",
    "            transforms.Resize(image_size),\n",
    "            transforms.ToTensor(),\n",
    "            transforms.Normalize([0.485, 0.456, 0.406],[0.229, 0.224, 0.225])\n",
    "        ])\n",
    "\n",
    "    return data_transformation"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "execution_count": 2,
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "source": [
    "## 設置客戶端模型及參數配置\n",
    "### 負責訓練，傳回訓練後梯度給中央端"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "outputs": [],
   "source": [
    "class client:\n",
    "    def __init__(self):\n",
    "        self.name = ''\n",
    "        self.net = models.resnet18(pretrained = True)\n",
    "        # Early-stopping 階段參數設置\n",
    "        self.batch_size = 64\n",
    "        self.epochs = 20\n",
    "        self.learning_rate = 0.001\n",
    "        self.criterion=nn.CrossEntropyLoss()\n",
    "        # 只訓練分類器\n",
    "        for param in self.net.parameters():\n",
    "            param.requires_grad = False\n",
    "        self.optimizer = torch.optim.Adamax(self.net.parameters(), lr=self.learning_rate)\n",
    "\n",
    "def training_dataset(self, training_path, image_size = (128,128), label = {}):\n",
    "    trainset = datasets.ImageFolder(training_path, transform=images_transforms('training', image_size))\n",
    "    # 定義標籤並更新\n",
    "    label_dict = {k: v for k, v in label.items() if k in trainset.class_to_idx}\n",
    "    trainset.class_to_idx.update(label_dict)\n",
    "    self.train_loader = DataLoader(trainset, batch_size=self.batch_size, shuffle=True, num_workers=2)\n",
    "client.training_dataset = training_dataset\n",
    "\n",
    "# 建構全連結層神經網路\n",
    "def net_linear(self, num_classes=4):\n",
    "    self.input = self.net.fc.in_features\n",
    "    self.net.fc = nn.Sequential(OrderedDict([\n",
    "        ('fc1', nn.Linear(self.input,1024)),\n",
    "        ('relu1', nn.ReLU(inplace=True)),\n",
    "        #('dropout1', nn.Dropout(p=0.5)),\n",
    "        ('fc2', nn.Linear(1024,num_classes)),\n",
    "    ]))\n",
    "client.net_linear = net_linear"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "## 設置中央服務端模型及參數配置\n",
    "### 負責訓練後梯度交換計算，以及測試"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "outputs": [],
   "source": [
    "class server:\n",
    "    def __init__(self):\n",
    "        self.global_model = models.resnet18(pretrained = True)\n",
    "        self.testing_path = 'data/Testing'\n",
    "        self.batch_size = 64\n",
    "        # 定義各病徵標籤\n",
    "        self.label = {'no_tumor': 0, 'meningioma_tumor': 1, 'glioma_tumor': 2, 'pituitary_tumor': 3}\n",
    "        # 只訓練分類器\n",
    "        for param in self.global_model.parameters():\n",
    "            param.requires_grad = False\n",
    "    # model aggregate\n",
    "    def FedAvg(self, clients):\n",
    "        # 聚合\n",
    "        weight_accumulator = dict()\n",
    "        # 形狀構建\n",
    "        for name, params in self.global_model.state_dict().items():\n",
    "            weight_accumulator[name] = torch.zeros_like(params)\n",
    "\n",
    "        # 將不同模型之神經層參數與中央server模型取差值\n",
    "        for client in clients:\n",
    "            # 差值\n",
    "            diff = dict()\n",
    "            for name, data in client.net.state_dict().items():\n",
    "                diff[name] = (data - self.global_model.state_dict()[name])\n",
    "            for name, params in self.global_model.state_dict().items():\n",
    "                weight_accumulator[name].add_((diff[name]/len(clients)).long())\n",
    "\n",
    "        # 將不同模型之神經層參數權重平均後聚合\n",
    "        for name, params in self.global_model.state_dict().items():\n",
    "            params.add_(weight_accumulator[name])\n",
    "\n",
    "        # 將參數權重平均後總和的server端模型更新到各client端模型之參數\n",
    "        for client in clients:\n",
    "            for name, params in self.global_model.state_dict().items():\n",
    "                client.net.state_dict()[name].copy_(params)\n",
    "# testset\n",
    "def testing_dataset(self, image_size = (128,128)):\n",
    "    testset = datasets.ImageFolder(self.testing_path, transform=images_transforms('test', image_size))\n",
    "    # 定義標籤並更新\n",
    "    label_dict = {k: v for k, v in self.label.items() if k in testset.class_to_idx}\n",
    "    testset.class_to_idx.update(label_dict)\n",
    "    testset, valset = torch.utils.data.random_split(testset, [150, 244])\n",
    "    self.test_loader = DataLoader(testset, batch_size=self.batch_size, shuffle=True, num_workers=2)\n",
    "    self.val_loader = DataLoader(valset, batch_size=self.batch_size, shuffle=True, num_workers=2)\n",
    "server.testing_dataset = testing_dataset\n",
    "# 建構全連結層神經網路\n",
    "def net_linear(self, num_classes=4):\n",
    "    self.input = self.global_model.fc.in_features\n",
    "    self.global_model.fc = nn.Sequential(OrderedDict([\n",
    "        ('fc1', nn.Linear(self.input,1024)),\n",
    "        ('relu1', nn.ReLU(inplace=True)),\n",
    "        #('dropout1', nn.Dropout(p=0.5)),\n",
    "        ('fc2', nn.Linear(1024,num_classes)),\n",
    "    ]))\n",
    "server.net_linear = net_linear"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "## 初始化模型\n",
    "### 設置參數"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "outputs": [],
   "source": [
    "# model A\n",
    "training_path_A = 'data/Training_A/'\n",
    "# model B\n",
    "training_path_B = 'data/Training_B/'\n",
    "# model C\n",
    "training_path_C = 'data/Training_C/'\n",
    "\n",
    "# 定義各病徵標籤\n",
    "label = {'no_tumor': 0, 'meningioma_tumor': 1, 'glioma_tumor': 2, 'pituitary_tumor': 3}\n",
    "\n",
    "exchange_epochs = 30 # 交換資訊次數\n",
    "\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "### 模型建置及資料集設置"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "outputs": [],
   "source": [
    "A = client()\n",
    "A.name = 'meningioma_tumor'\n",
    "A.training_dataset(training_path=training_path_A, label=label)\n",
    "A.net_linear(num_classes=len(label))\n",
    "A.net = A.net.to(device)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "outputs": [],
   "source": [
    "B = client()\n",
    "B.name = 'glioma_tumor'\n",
    "B.training_dataset(training_path=training_path_B, label=label)\n",
    "B.net_linear(num_classes=len(label))\n",
    "B.net = B.net.to(device)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "outputs": [],
   "source": [
    "C = client()\n",
    "C.name = 'pituitary_tumor'\n",
    "C.training_dataset(training_path=training_path_C, label=label)\n",
    "C.net_linear(num_classes=len(label))\n",
    "C.net = C.net.to(device)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "outputs": [],
   "source": [
    "# 中央計算建置\n",
    "main = server()\n",
    "main.testing_dataset()\n",
    "main.net_linear()\n",
    "main.global_model = main.global_model.to(device)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "## 訓練模型，并印出損失函數"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "outputs": [],
   "source": [
    "def local_train(model, train_loader, criterion, optimizer, epochs=10):\n",
    "    for e in range(epochs):\n",
    "        # Train the model\n",
    "        model.train()\n",
    "        for i, (images, labels) in enumerate(train_loader):\n",
    "            images = images.to(device)\n",
    "            labels = labels.to(device)\n",
    "\n",
    "            # Forward pass\n",
    "            outputs = model(images)\n",
    "            loss = criterion(outputs, labels)\n",
    "\n",
    "            # Backward and optimize\n",
    "            optimizer.zero_grad()\n",
    "            loss.backward()\n",
    "            optimizer.step()"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "outputs": [],
   "source": [
    "# models : list\n",
    "def test(models, test_loader):\n",
    "    with torch.no_grad():\n",
    "        acc = {}\n",
    "        # 每個模型測試\n",
    "        for model in models:\n",
    "            y_pred = []\n",
    "            y_actual = []\n",
    "            model.net.eval()\n",
    "            for i, (images,labels) in enumerate(test_loader):\n",
    "                images = images.to(device)\n",
    "                labels = labels.to(device)\n",
    "\n",
    "                outputs = model.net(images)\n",
    "\n",
    "                y_actual += list(np.array(labels.detach().to('cpu')).flatten())\n",
    "                predictes = torch.max(outputs, 1)[1]\n",
    "                y_pred += list(np.array(predictes.detach().to('cpu')).flatten())\n",
    "\n",
    "            y_actual = np.array(y_actual).flatten()\n",
    "            y_pred = np.array(y_pred).flatten()\n",
    "            acc[model.name]=accuracy_score(y_actual, y_pred)\n",
    "\n",
    "        return acc"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "testing before exchange:\n",
      "exchange_epoch : 1\n",
      "meningioma_tumor : 0.3466666666666667\n",
      "glioma_tumor : 0.23333333333333334\n",
      "pituitary_tumor : 0.16666666666666666\n",
      "testing after exchange:\n",
      "exchange_epoch : 1\n",
      "meningioma_tumor : 0.26\n",
      "glioma_tumor : 0.26\n",
      "pituitary_tumor : 0.26\n",
      "testing before exchange:\n",
      "exchange_epoch : 2\n",
      "meningioma_tumor : 0.26\n",
      "glioma_tumor : 0.2733333333333333\n",
      "pituitary_tumor : 0.24666666666666667\n",
      "testing after exchange:\n",
      "exchange_epoch : 2\n",
      "meningioma_tumor : 0.26\n",
      "glioma_tumor : 0.26\n",
      "pituitary_tumor : 0.26\n",
      "testing before exchange:\n",
      "exchange_epoch : 3\n",
      "meningioma_tumor : 0.2733333333333333\n",
      "glioma_tumor : 0.2733333333333333\n",
      "pituitary_tumor : 0.24666666666666667\n",
      "testing after exchange:\n",
      "exchange_epoch : 3\n",
      "meningioma_tumor : 0.26\n",
      "glioma_tumor : 0.26\n",
      "pituitary_tumor : 0.26\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001B[1;31m---------------------------------------------------------------------------\u001B[0m",
      "\u001B[1;31mKeyboardInterrupt\u001B[0m                         Traceback (most recent call last)",
      "Cell \u001B[1;32mIn [12], line 13\u001B[0m\n\u001B[0;32m     10\u001B[0m \u001B[38;5;28;01mfor\u001B[39;00m exchange_epoch \u001B[38;5;129;01min\u001B[39;00m \u001B[38;5;28mrange\u001B[39m(exchange_epochs):\n\u001B[0;32m     11\u001B[0m     \u001B[38;5;66;03m# federated train\u001B[39;00m\n\u001B[0;32m     12\u001B[0m     \u001B[38;5;28;01mfor\u001B[39;00m model \u001B[38;5;129;01min\u001B[39;00m models:\n\u001B[1;32m---> 13\u001B[0m         \u001B[43mlocal_train\u001B[49m\u001B[43m(\u001B[49m\u001B[43mmodel\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mnet\u001B[49m\u001B[43m,\u001B[49m\u001B[43mmodel\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mtrain_loader\u001B[49m\u001B[43m,\u001B[49m\u001B[43mmodel\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mcriterion\u001B[49m\u001B[43m,\u001B[49m\u001B[43mmodel\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43moptimizer\u001B[49m\u001B[43m,\u001B[49m\u001B[43mmodel\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mepochs\u001B[49m\u001B[43m)\u001B[49m\n\u001B[0;32m     15\u001B[0m     \u001B[38;5;66;03m# testing before exchange\u001B[39;00m\n\u001B[0;32m     16\u001B[0m     \u001B[38;5;28mprint\u001B[39m(\u001B[38;5;124mf\u001B[39m\u001B[38;5;124m'\u001B[39m\u001B[38;5;124mtesting before exchange:\u001B[39m\u001B[38;5;124m'\u001B[39m)\n",
      "Cell \u001B[1;32mIn [10], line 5\u001B[0m, in \u001B[0;36mlocal_train\u001B[1;34m(model, train_loader, criterion, optimizer, epochs)\u001B[0m\n\u001B[0;32m      2\u001B[0m \u001B[38;5;28;01mfor\u001B[39;00m e \u001B[38;5;129;01min\u001B[39;00m \u001B[38;5;28mrange\u001B[39m(epochs):\n\u001B[0;32m      3\u001B[0m     \u001B[38;5;66;03m# Train the model\u001B[39;00m\n\u001B[0;32m      4\u001B[0m     model\u001B[38;5;241m.\u001B[39mtrain()\n\u001B[1;32m----> 5\u001B[0m     \u001B[38;5;28;01mfor\u001B[39;00m i, (images, labels) \u001B[38;5;129;01min\u001B[39;00m \u001B[38;5;28menumerate\u001B[39m(train_loader):\n\u001B[0;32m      6\u001B[0m         images \u001B[38;5;241m=\u001B[39m images\u001B[38;5;241m.\u001B[39mto(device)\n\u001B[0;32m      7\u001B[0m         labels \u001B[38;5;241m=\u001B[39m labels\u001B[38;5;241m.\u001B[39mto(device)\n",
      "File \u001B[1;32m~\\Python39\\lib\\site-packages\\torch\\utils\\data\\dataloader.py:435\u001B[0m, in \u001B[0;36m_BaseDataLoaderIter.__next__\u001B[1;34m(self)\u001B[0m\n\u001B[0;32m    433\u001B[0m \u001B[38;5;28;01mif\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_sampler_iter \u001B[38;5;129;01mis\u001B[39;00m \u001B[38;5;28;01mNone\u001B[39;00m:\n\u001B[0;32m    434\u001B[0m     \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_reset()\n\u001B[1;32m--> 435\u001B[0m data \u001B[38;5;241m=\u001B[39m \u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43m_next_data\u001B[49m\u001B[43m(\u001B[49m\u001B[43m)\u001B[49m\n\u001B[0;32m    436\u001B[0m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_num_yielded \u001B[38;5;241m+\u001B[39m\u001B[38;5;241m=\u001B[39m \u001B[38;5;241m1\u001B[39m\n\u001B[0;32m    437\u001B[0m \u001B[38;5;28;01mif\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_dataset_kind \u001B[38;5;241m==\u001B[39m _DatasetKind\u001B[38;5;241m.\u001B[39mIterable \u001B[38;5;129;01mand\u001B[39;00m \\\n\u001B[0;32m    438\u001B[0m         \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_IterableDataset_len_called \u001B[38;5;129;01mis\u001B[39;00m \u001B[38;5;129;01mnot\u001B[39;00m \u001B[38;5;28;01mNone\u001B[39;00m \u001B[38;5;129;01mand\u001B[39;00m \\\n\u001B[0;32m    439\u001B[0m         \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_num_yielded \u001B[38;5;241m>\u001B[39m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_IterableDataset_len_called:\n",
      "File \u001B[1;32m~\\Python39\\lib\\site-packages\\torch\\utils\\data\\dataloader.py:1068\u001B[0m, in \u001B[0;36m_MultiProcessingDataLoaderIter._next_data\u001B[1;34m(self)\u001B[0m\n\u001B[0;32m   1065\u001B[0m     \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_process_data(data)\n\u001B[0;32m   1067\u001B[0m \u001B[38;5;28;01massert\u001B[39;00m \u001B[38;5;129;01mnot\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_shutdown \u001B[38;5;129;01mand\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_tasks_outstanding \u001B[38;5;241m>\u001B[39m \u001B[38;5;241m0\u001B[39m\n\u001B[1;32m-> 1068\u001B[0m idx, data \u001B[38;5;241m=\u001B[39m \u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43m_get_data\u001B[49m\u001B[43m(\u001B[49m\u001B[43m)\u001B[49m\n\u001B[0;32m   1069\u001B[0m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_tasks_outstanding \u001B[38;5;241m-\u001B[39m\u001B[38;5;241m=\u001B[39m \u001B[38;5;241m1\u001B[39m\n\u001B[0;32m   1070\u001B[0m \u001B[38;5;28;01mif\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_dataset_kind \u001B[38;5;241m==\u001B[39m _DatasetKind\u001B[38;5;241m.\u001B[39mIterable:\n\u001B[0;32m   1071\u001B[0m     \u001B[38;5;66;03m# Check for _IterableDatasetStopIteration\u001B[39;00m\n",
      "File \u001B[1;32m~\\Python39\\lib\\site-packages\\torch\\utils\\data\\dataloader.py:1034\u001B[0m, in \u001B[0;36m_MultiProcessingDataLoaderIter._get_data\u001B[1;34m(self)\u001B[0m\n\u001B[0;32m   1030\u001B[0m     \u001B[38;5;66;03m# In this case, `self._data_queue` is a `queue.Queue`,. But we don't\u001B[39;00m\n\u001B[0;32m   1031\u001B[0m     \u001B[38;5;66;03m# need to call `.task_done()` because we don't use `.join()`.\u001B[39;00m\n\u001B[0;32m   1032\u001B[0m \u001B[38;5;28;01melse\u001B[39;00m:\n\u001B[0;32m   1033\u001B[0m     \u001B[38;5;28;01mwhile\u001B[39;00m \u001B[38;5;28;01mTrue\u001B[39;00m:\n\u001B[1;32m-> 1034\u001B[0m         success, data \u001B[38;5;241m=\u001B[39m \u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43m_try_get_data\u001B[49m\u001B[43m(\u001B[49m\u001B[43m)\u001B[49m\n\u001B[0;32m   1035\u001B[0m         \u001B[38;5;28;01mif\u001B[39;00m success:\n\u001B[0;32m   1036\u001B[0m             \u001B[38;5;28;01mreturn\u001B[39;00m data\n",
      "File \u001B[1;32m~\\Python39\\lib\\site-packages\\torch\\utils\\data\\dataloader.py:872\u001B[0m, in \u001B[0;36m_MultiProcessingDataLoaderIter._try_get_data\u001B[1;34m(self, timeout)\u001B[0m\n\u001B[0;32m    859\u001B[0m \u001B[38;5;28;01mdef\u001B[39;00m \u001B[38;5;21m_try_get_data\u001B[39m(\u001B[38;5;28mself\u001B[39m, timeout\u001B[38;5;241m=\u001B[39m_utils\u001B[38;5;241m.\u001B[39mMP_STATUS_CHECK_INTERVAL):\n\u001B[0;32m    860\u001B[0m     \u001B[38;5;66;03m# Tries to fetch data from `self._data_queue` once for a given timeout.\u001B[39;00m\n\u001B[0;32m    861\u001B[0m     \u001B[38;5;66;03m# This can also be used as inner loop of fetching without timeout, with\u001B[39;00m\n\u001B[1;32m   (...)\u001B[0m\n\u001B[0;32m    869\u001B[0m     \u001B[38;5;66;03m# Returns a 2-tuple:\u001B[39;00m\n\u001B[0;32m    870\u001B[0m     \u001B[38;5;66;03m#   (bool: whether successfully get data, any: data if successful else None)\u001B[39;00m\n\u001B[0;32m    871\u001B[0m     \u001B[38;5;28;01mtry\u001B[39;00m:\n\u001B[1;32m--> 872\u001B[0m         data \u001B[38;5;241m=\u001B[39m \u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43m_data_queue\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mget\u001B[49m\u001B[43m(\u001B[49m\u001B[43mtimeout\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43mtimeout\u001B[49m\u001B[43m)\u001B[49m\n\u001B[0;32m    873\u001B[0m         \u001B[38;5;28;01mreturn\u001B[39;00m (\u001B[38;5;28;01mTrue\u001B[39;00m, data)\n\u001B[0;32m    874\u001B[0m     \u001B[38;5;28;01mexcept\u001B[39;00m \u001B[38;5;167;01mException\u001B[39;00m \u001B[38;5;28;01mas\u001B[39;00m e:\n\u001B[0;32m    875\u001B[0m         \u001B[38;5;66;03m# At timeout and error, we manually check whether any worker has\u001B[39;00m\n\u001B[0;32m    876\u001B[0m         \u001B[38;5;66;03m# failed. Note that this is the only mechanism for Windows to detect\u001B[39;00m\n\u001B[0;32m    877\u001B[0m         \u001B[38;5;66;03m# worker failures.\u001B[39;00m\n",
      "File \u001B[1;32m~\\Python39\\lib\\multiprocessing\\queues.py:113\u001B[0m, in \u001B[0;36mQueue.get\u001B[1;34m(self, block, timeout)\u001B[0m\n\u001B[0;32m    111\u001B[0m \u001B[38;5;28;01mif\u001B[39;00m block:\n\u001B[0;32m    112\u001B[0m     timeout \u001B[38;5;241m=\u001B[39m deadline \u001B[38;5;241m-\u001B[39m time\u001B[38;5;241m.\u001B[39mmonotonic()\n\u001B[1;32m--> 113\u001B[0m     \u001B[38;5;28;01mif\u001B[39;00m \u001B[38;5;129;01mnot\u001B[39;00m \u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43m_poll\u001B[49m\u001B[43m(\u001B[49m\u001B[43mtimeout\u001B[49m\u001B[43m)\u001B[49m:\n\u001B[0;32m    114\u001B[0m         \u001B[38;5;28;01mraise\u001B[39;00m Empty\n\u001B[0;32m    115\u001B[0m \u001B[38;5;28;01melif\u001B[39;00m \u001B[38;5;129;01mnot\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_poll():\n",
      "File \u001B[1;32m~\\Python39\\lib\\multiprocessing\\connection.py:262\u001B[0m, in \u001B[0;36m_ConnectionBase.poll\u001B[1;34m(self, timeout)\u001B[0m\n\u001B[0;32m    260\u001B[0m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_check_closed()\n\u001B[0;32m    261\u001B[0m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_check_readable()\n\u001B[1;32m--> 262\u001B[0m \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43m_poll\u001B[49m\u001B[43m(\u001B[49m\u001B[43mtimeout\u001B[49m\u001B[43m)\u001B[49m\n",
      "File \u001B[1;32m~\\Python39\\lib\\multiprocessing\\connection.py:335\u001B[0m, in \u001B[0;36mPipeConnection._poll\u001B[1;34m(self, timeout)\u001B[0m\n\u001B[0;32m    332\u001B[0m \u001B[38;5;28;01mif\u001B[39;00m (\u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_got_empty_message \u001B[38;5;129;01mor\u001B[39;00m\n\u001B[0;32m    333\u001B[0m             _winapi\u001B[38;5;241m.\u001B[39mPeekNamedPipe(\u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_handle)[\u001B[38;5;241m0\u001B[39m] \u001B[38;5;241m!=\u001B[39m \u001B[38;5;241m0\u001B[39m):\n\u001B[0;32m    334\u001B[0m     \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[38;5;28;01mTrue\u001B[39;00m\n\u001B[1;32m--> 335\u001B[0m \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[38;5;28mbool\u001B[39m(\u001B[43mwait\u001B[49m\u001B[43m(\u001B[49m\u001B[43m[\u001B[49m\u001B[38;5;28;43mself\u001B[39;49m\u001B[43m]\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mtimeout\u001B[49m\u001B[43m)\u001B[49m)\n",
      "File \u001B[1;32m~\\Python39\\lib\\multiprocessing\\connection.py:884\u001B[0m, in \u001B[0;36mwait\u001B[1;34m(object_list, timeout)\u001B[0m\n\u001B[0;32m    881\u001B[0m                 ready_objects\u001B[38;5;241m.\u001B[39madd(o)\n\u001B[0;32m    882\u001B[0m                 timeout \u001B[38;5;241m=\u001B[39m \u001B[38;5;241m0\u001B[39m\n\u001B[1;32m--> 884\u001B[0m     ready_handles \u001B[38;5;241m=\u001B[39m \u001B[43m_exhaustive_wait\u001B[49m\u001B[43m(\u001B[49m\u001B[43mwaithandle_to_obj\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mkeys\u001B[49m\u001B[43m(\u001B[49m\u001B[43m)\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mtimeout\u001B[49m\u001B[43m)\u001B[49m\n\u001B[0;32m    885\u001B[0m \u001B[38;5;28;01mfinally\u001B[39;00m:\n\u001B[0;32m    886\u001B[0m     \u001B[38;5;66;03m# request that overlapped reads stop\u001B[39;00m\n\u001B[0;32m    887\u001B[0m     \u001B[38;5;28;01mfor\u001B[39;00m ov \u001B[38;5;129;01min\u001B[39;00m ov_list:\n",
      "File \u001B[1;32m~\\Python39\\lib\\multiprocessing\\connection.py:816\u001B[0m, in \u001B[0;36m_exhaustive_wait\u001B[1;34m(handles, timeout)\u001B[0m\n\u001B[0;32m    814\u001B[0m ready \u001B[38;5;241m=\u001B[39m []\n\u001B[0;32m    815\u001B[0m \u001B[38;5;28;01mwhile\u001B[39;00m L:\n\u001B[1;32m--> 816\u001B[0m     res \u001B[38;5;241m=\u001B[39m \u001B[43m_winapi\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mWaitForMultipleObjects\u001B[49m\u001B[43m(\u001B[49m\u001B[43mL\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;28;43;01mFalse\u001B[39;49;00m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mtimeout\u001B[49m\u001B[43m)\u001B[49m\n\u001B[0;32m    817\u001B[0m     \u001B[38;5;28;01mif\u001B[39;00m res \u001B[38;5;241m==\u001B[39m WAIT_TIMEOUT:\n\u001B[0;32m    818\u001B[0m         \u001B[38;5;28;01mbreak\u001B[39;00m\n",
      "\u001B[1;31mKeyboardInterrupt\u001B[0m: "
     ]
    }
   ],
   "source": [
    "models = [A, B, C]\n",
    "# log before exchange\n",
    "before_epoch = []\n",
    "before_model = {f'{model.name}':[] for model in models}\n",
    "before_acc = {}\n",
    "# log after exchange\n",
    "after_epoch = []\n",
    "after_model = {f'{model.name}':[] for model in models}\n",
    "after_acc = {}\n",
    "for exchange_epoch in range(exchange_epochs):\n",
    "    # federated train\n",
    "    for model in models:\n",
    "        local_train(model.net,model.train_loader,model.criterion,model.optimizer,model.epochs)\n",
    "\n",
    "    # testing before exchange\n",
    "    print(f'testing before exchange:')\n",
    "    before_acc = test(models, main.test_loader)\n",
    "\n",
    "    before_epoch.append(str(exchange_epoch+1))\n",
    "    for model in models:\n",
    "        before_model[model.name].append(str(before_acc[model.name]))\n",
    "    # 列出準確率\n",
    "    print(f'exchange_epoch : {exchange_epoch+1}')\n",
    "    for model in models:\n",
    "        print(f'{model.name} : {before_acc[model.name]}')\n",
    "\n",
    "    # exchange\n",
    "    main.FedAvg(models)\n",
    "\n",
    "    # testing after exchange\n",
    "    print(f'testing after exchange:')\n",
    "    after_acc = test(models, main.test_loader)\n",
    "\n",
    "    after_epoch.append(str(exchange_epoch+1))\n",
    "    for model in models:\n",
    "        after_model[model.name].append(str(after_acc[model.name]))\n",
    "    # 列出準確率\n",
    "    print(f'exchange_epoch : {exchange_epoch+1}')\n",
    "    for model in models:\n",
    "        print(f'{model.name} : {after_acc[model.name]}')\n",
    "\n",
    "# save before log\n",
    "df = pd.DataFrame (before_epoch,columns=['epoch'])\n",
    "for model in models:\n",
    "    df[model.name] = before_model[model.name]\n",
    "\n",
    "df.to_csv ('outputs/fedavg_model_before.csv', index = False, header=True)\n",
    "# save after log\n",
    "df = pd.DataFrame (after_epoch,columns=['epoch'])\n",
    "for model in models:\n",
    "    df[model.name] = after_model[model.name]\n",
    "\n",
    "df.to_csv ('outputs/fedavg_model_after.csv', index = False, header=True)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "raw",
   "source": [
    "#model.load_state_dict(torch.load('model\\\\mobilenet_v3_large.pt'))\n",
    "y_actual,y_pred = test(model, test_loader)\n",
    "acc = classification_report(y_actual,y_pred,target_names=trainset.classes)\n",
    "df = pd.DataFrame(classification_report(y_actual,y_pred,target_names=trainset.classes, output_dict=True)).transpose()\n",
    "df.to_csv(\"outputs\\\\mobilenet_v3_large_result.csv\", index= True)\n",
    "print(f\"{acc}\")"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% raw\n"
    }
   }
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  },
  "colab": {
   "name": "dog&cat_ai_cnn.ipynb",
   "provenance": [],
   "collapsed_sections": []
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}